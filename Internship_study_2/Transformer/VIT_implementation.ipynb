{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"VIT_implementation.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOUYsaVIM1XCAdMPlc7gFUK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## https://yhkim4504.tistory.com/5\n","## 이곳을 참고하여 작성했습니다."],"metadata":{"id":"amo4m0JZxqIJ"}},{"cell_type":"code","source":["!pip install einops"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PIE4lI5uxeFV","executionInfo":{"status":"ok","timestamp":1642082994330,"user_tz":-540,"elapsed":4388,"user":{"displayName":"ᄋᄋᄌ","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07872107978641370856"}},"outputId":"11848842-5097-4968-f894-969e26a4b262"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting einops\n","  Downloading einops-0.3.2-py3-none-any.whl (25 kB)\n","Installing collected packages: einops\n","Successfully installed einops-0.3.2\n"]}]},{"cell_type":"code","execution_count":6,"metadata":{"id":"WaDg6brYwdWL","executionInfo":{"status":"ok","timestamp":1642083001389,"user_tz":-540,"elapsed":266,"user":{"displayName":"ᄋᄋᄌ","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07872107978641370856"}}},"outputs":[],"source":["import torch\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","\n","from torch import nn\n","from torch import Tensor\n","from PIL import Image\n","from torchvision.transforms import Compose, Resize, ToTensor\n","from einops import rearrange, reduce, repeat\n","from einops.layers.torch import Rearrange, Reduce\n","from torchsummary import summary"]},{"cell_type":"markdown","source":["(B,C,H,W) = (8,3,224,224)를 갖는 랜덤 텐서를 사용하여 텐서연산과정 관찰"],"metadata":{"id":"MtcirK67xpOn"}},{"cell_type":"code","source":["x = torch.randn(8, 3, 224, 224)\n","x.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZWsVQXtdws2l","executionInfo":{"status":"ok","timestamp":1642085007235,"user_tz":-540,"elapsed":324,"user":{"displayName":"ᄋᄋᄌ","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07872107978641370856"}},"outputId":"90442394-2a1d-41d7-dea5-fc0c4c6977a3"},"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([8, 3, 224, 224])"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["patch_size = 16\n","in_channels = 3\n","emb_size = 768\n","\n","projection = nn.Sequential(\n","            # using a conv layer instead of a linear one -> performance gains\n","            nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size),\n","            Rearrange('b e (h) (w) -> b (h w) e'),\n","        )\n","\n","projection(x).shape # (B, N, P*P*C)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KQTKZQ7WyA_f","executionInfo":{"status":"ok","timestamp":1642083460133,"user_tz":-540,"elapsed":315,"user":{"displayName":"ᄋᄋᄌ","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07872107978641370856"}},"outputId":"a41d247d-d8da-48a8-f90d-dc29822f9390"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([8, 196, 768])"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["## **CLS Token, Positional Encoding**"],"metadata":{"id":"lUj2tXm4zUv2"}},{"cell_type":"code","source":["emb_size = 768\n","img_size = 224\n","patch_size = 16\n","\n","# 이미지를 패치사이즈로 나누고 flatten\n","projected_x = projection(x)\n","print('Projected X shape :', projected_x.shape)\n","\n","# cls_token과 pos encoding Parameter 정의\n","cls_token = nn.Parameter(torch.randn(1,1, emb_size))\n","positions = nn.Parameter(torch.randn((img_size // patch_size) **2 + 1, emb_size))\n","print('Cls Shape :', cls_token.shape, ', Pos Shape :', positions.shape)\n","\n","# cls_token을 반복하여 배치사이즈의 크기와 맞춰줌\n","batch_size = 8\n","cls_tokens = repeat(cls_token, '() n e -> b n e', b=batch_size)\n","print('Repeated Cls shape :', cls_tokens.shape)\n","\n","# cls_token과 projected_x를 concatenate\n","cat_x = torch.cat([cls_tokens, projected_x], dim=1)\n","\n","# position encoding을 더해줌\n","cat_x += positions\n","print('output : ', cat_x.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1zBVS0evzT3h","executionInfo":{"status":"ok","timestamp":1642083493286,"user_tz":-540,"elapsed":267,"user":{"displayName":"ᄋᄋᄌ","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07872107978641370856"}},"outputId":"f0177c37-a4b1-431a-cb6e-3b05a4f2a0eb"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Projected X shape : torch.Size([8, 196, 768])\n","Cls Shape : torch.Size([1, 1, 768]) , Pos Shape : torch.Size([197, 768])\n","Repeated Cls shape : torch.Size([8, 1, 768])\n","output :  torch.Size([8, 197, 768])\n"]}]},{"cell_type":"markdown","source":["위 셀을 class로 만들어주면 아래와 같습니다"],"metadata":{"id":"JmToOucM2B8M"}},{"cell_type":"code","source":["class PatchEmbedding(nn.Module):\n","    def __init__(self, in_channels: int = 3, patch_size: int = 16, emb_size: int = 768, img_size: int = 224):\n","        self.patch_size = patch_size\n","        super().__init__()\n","        self.projection = nn.Sequential(\n","            # using a conv layer instead of a linear one -> performance gains\n","            nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size),\n","            Rearrange('b e (h) (w) -> b (h w) e'),\n","        )\n","        self.cls_token = nn.Parameter(torch.randn(1,1, emb_size))\n","        self.positions = nn.Parameter(torch.randn((img_size // patch_size) **2 + 1, emb_size))\n","        \n","    def forward(self, x: Tensor) -> Tensor:\n","        b, _, _, _ = x.shape\n","        x = self.projection(x)\n","        cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=b)\n","        # prepend the cls token to the input\n","        x = torch.cat([cls_tokens, x], dim=1)\n","        # add position embedding\n","        x += self.positions\n","\n","        return x"],"metadata":{"id":"09yyBVm70ShM","executionInfo":{"status":"ok","timestamp":1642083932525,"user_tz":-540,"elapsed":269,"user":{"displayName":"ᄋᄋᄌ","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07872107978641370856"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["pathEmbed = PatchEmbedding()\n","x = pathEmbed(x)\n","x.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mATgQzos1B0E","executionInfo":{"status":"ok","timestamp":1642085011059,"user_tz":-540,"elapsed":284,"user":{"displayName":"ᄋᄋᄌ","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07872107978641370856"}},"outputId":"ca27dc8a-9a8e-4fd7-fbd3-9e0c9c2d97a7"},"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([8, 197, 768])"]},"metadata":{},"execution_count":39}]},{"cell_type":"markdown","source":["## **Linear Projection**"],"metadata":{"id":"ly2gJ-_U0XZx"}},{"cell_type":"code","source":["emb_size = 768\n","num_heads = 8\n","\n","keys = nn.Linear(emb_size, emb_size)\n","queries = nn.Linear(emb_size, emb_size)\n","values = nn.Linear(emb_size, emb_size)\n","print(keys, queries, values, sep='\\n')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3lo3v4zE0Zsc","executionInfo":{"status":"ok","timestamp":1642084219700,"user_tz":-540,"elapsed":292,"user":{"displayName":"ᄋᄋᄌ","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07872107978641370856"}},"outputId":"d4862f36-5ea2-4449-a2cb-164ae06bb5a1"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Linear(in_features=768, out_features=768, bias=True)\n","Linear(in_features=768, out_features=768, bias=True)\n","Linear(in_features=768, out_features=768, bias=True)\n"]}]},{"cell_type":"markdown","source":["## **Multi-Head**"],"metadata":{"id":"_GXr-N000rKz"}},{"cell_type":"code","source":["queries = rearrange(queries(x), \"b n (h d) -> b h n d\", h=num_heads)\n","keys = rearrange(keys(x), \"b n (h d) -> b h n d\", h=num_heads)\n","values  = rearrange(values(x), \"b n (h d) -> b h n d\", h=num_heads)\n","\n","print('[shape]', queries.shape, keys.shape, values.shape, sep='\\n')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VCWyXcLo0uRa","executionInfo":{"status":"ok","timestamp":1642084221974,"user_tz":-540,"elapsed":268,"user":{"displayName":"ᄋᄋᄌ","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07872107978641370856"}},"outputId":"ed8069d3-f8b7-4f40-bd7e-7ecd3f04859a"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["[shape]\n","torch.Size([8, 8, 197, 96])\n","torch.Size([8, 8, 197, 96])\n","torch.Size([8, 8, 197, 96])\n"]}]},{"cell_type":"markdown","source":["## **scaled dot-product attention**"],"metadata":{"id":"sfDGubFq2zrL"}},{"cell_type":"markdown","source":["Attention(Q,K,V) = softmax(QK^T/sqrt(dk))*V"],"metadata":{"id":"WJoyIrW43N1D"}},{"cell_type":"code","source":["# Queries * Keys\n","energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)\n","print('energy :', energy.shape)\n","\n","# Get Attention Score\n","scaling = emb_size ** (1/2)\n","att = F.softmax(energy, dim=-1) / scaling\n","print('att :', att.shape)\n","\n","# Attention Score * values\n","out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n","print('out :', out.shape)\n","\n","# Rearrage to emb_size\n","out = rearrange(out, \"b h n d -> b n (h d)\")\n","print('out2 : ', out.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nVn1xc3e2y4D","executionInfo":{"status":"ok","timestamp":1642084406998,"user_tz":-540,"elapsed":319,"user":{"displayName":"ᄋᄋᄌ","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07872107978641370856"}},"outputId":"b35c5900-8051-45e6-90c8-4adff1ec8cd6"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["energy : torch.Size([8, 8, 197, 197])\n","att : torch.Size([8, 8, 197, 197])\n","out : torch.Size([8, 8, 197, 96])\n","out2 :  torch.Size([8, 197, 768])\n"]}]},{"cell_type":"markdown","source":["위 셀을 class로 구현하면 아래와 같습니다."],"metadata":{"id":"94r-dVxb3mbS"}},{"cell_type":"code","source":["class MultiHeadAttention(nn.Module):\n","    def __init__(self, emb_size: int = 768, num_heads: int = 8, dropout: float = 0):\n","        super().__init__()\n","        self.emb_size = emb_size\n","        self.num_heads = num_heads\n","        # fuse the queries, keys and values in one matrix\n","        self.qkv = nn.Linear(emb_size, emb_size * 3)\n","        self.att_drop = nn.Dropout(dropout)\n","        self.projection = nn.Linear(emb_size, emb_size)\n","        \n","    def forward(self, x : Tensor, mask: Tensor = None) -> Tensor:\n","        # split keys, queries and values in num_heads\n","        qkv = rearrange(self.qkv(x), \"b n (h d qkv) -> (qkv) b h n d\", h=self.num_heads, qkv=3)\n","        queries, keys, values = qkv[0], qkv[1], qkv[2]\n","        # sum up over the last axis\n","        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys) # batch, num_heads, query_len, key_len\n","        if mask is not None:\n","            fill_value = torch.finfo(torch.float32).min\n","            energy.mask_fill(~mask, fill_value)\n","            \n","        scaling = self.emb_size ** (1/2)\n","        att = F.softmax(energy, dim=-1) / scaling\n","        att = self.att_drop(att)\n","        # sum up over the third axis\n","        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n","        out = rearrange(out, \"b h n d -> b n (h d)\")\n","        out = self.projection(out)\n","        return out"],"metadata":{"id":"_ZLsa6Ky3o9L","executionInfo":{"status":"ok","timestamp":1642084608822,"user_tz":-540,"elapsed":306,"user":{"displayName":"ᄋᄋᄌ","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07872107978641370856"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["MHA = MultiHeadAttention()\n","x = MHA(x)\n","print(x.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i3AGM1JR38iz","executionInfo":{"status":"ok","timestamp":1642085015412,"user_tz":-540,"elapsed":283,"user":{"displayName":"ᄋᄋᄌ","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07872107978641370856"}},"outputId":"4fe2548f-5ac9-4e50-f616-9b05fd51eac9"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([8, 197, 768])\n","torch.Size([8, 197, 768])\n"]}]},{"cell_type":"markdown","source":["## **Residual Block**"],"metadata":{"id":"aSYXoVRs4Ju5"}},{"cell_type":"code","source":["class ResidualAdd(nn.Module):\n","    def __init__(self, fn):\n","        super().__init__()\n","        self.fn = fn\n","        \n","    def forward(self, x, **kwargs):\n","        res = x\n","        x = self.fn(x, **kwargs)\n","        x += res\n","        return x"],"metadata":{"id":"XYUnmvrY4LWv","executionInfo":{"status":"ok","timestamp":1642084745943,"user_tz":-540,"elapsed":275,"user":{"displayName":"ᄋᄋᄌ","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07872107978641370856"}}},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":["## **Feed Forward MLP**"],"metadata":{"id":"n-4xN-8K4MiS"}},{"cell_type":"code","source":["class FeedForwardBlock(nn.Sequential):\n","    def __init__(self, emb_size: int, expansion: int = 4, drop_p: float = 0.):\n","        super().__init__(\n","            nn.Linear(emb_size, expansion * emb_size),\n","            nn.GELU(),\n","            nn.Dropout(drop_p),\n","            nn.Linear(expansion * emb_size, emb_size),\n","        )"],"metadata":{"id":"Ed6IneEx4PVM","executionInfo":{"status":"ok","timestamp":1642084777106,"user_tz":-540,"elapsed":294,"user":{"displayName":"ᄋᄋᄌ","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07872107978641370856"}}},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":["## **Transformer Encoder Block**"],"metadata":{"id":"xPOuNg1T4a0t"}},{"cell_type":"code","source":["class TransformerEncoderBlock(nn.Sequential):\n","    def __init__(self, emb_size: int = 768, drop_p: float = 0., forward_expansion: int = 4, \n","                 forward_drop_p: float = 0., ** kwargs):\n","        super().__init__(\n","            ResidualAdd(nn.Sequential(\n","                nn.LayerNorm(emb_size),\n","                MultiHeadAttention(emb_size, **kwargs),\n","                nn.Dropout(drop_p)\n","            )),\n","            ResidualAdd(nn.Sequential(\n","                nn.LayerNorm(emb_size),\n","                FeedForwardBlock(\n","                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n","                nn.Dropout(drop_p)\n","            )\n","        ))"],"metadata":{"id":"GwQqJC5p4dt3","executionInfo":{"status":"ok","timestamp":1642084932909,"user_tz":-540,"elapsed":282,"user":{"displayName":"ᄋᄋᄌ","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07872107978641370856"}}},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":["## **Block 쌓기**"],"metadata":{"id":"8x_CXmiG5T8E"}},{"cell_type":"markdown","source":["*[TransformerEncoderBlock(**kwargs) for _ in range(depth)] 에서 앞에 *이 붙은 이유는 인자를 리스트형식으로 보내는게 아니라 각각 나눠서 보내줘야되기 때문입니다.\\\n"," 예를 들면 인자를 [1,2,3]으로 넣을 경우 함수에서는 [1,2,3]으로 받지만 *[1, 2, 3]일 경우 1, 2, 3 으로 각각 나눠진 후 들어갑니다."],"metadata":{"id":"oL4IaQmG5iqm"}},{"cell_type":"code","source":["class TransformerEncoder(nn.Sequential):\n","    def __init__(self, depth: int = 12, **kwargs):\n","        super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])"],"metadata":{"id":"UJ1A3df35ZbF","executionInfo":{"status":"ok","timestamp":1642085069804,"user_tz":-540,"elapsed":264,"user":{"displayName":"ᄋᄋᄌ","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07872107978641370856"}}},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":["## **Head**"],"metadata":{"id":"CMKhjKaB5xE_"}},{"cell_type":"code","source":["class ClassificationHead(nn.Sequential):\n","    def __init__(self, emb_size: int = 768, n_classes: int = 1000):\n","        super().__init__(\n","            Reduce('b n e -> b e', reduction='mean'),\n","            nn.LayerNorm(emb_size), \n","            nn.Linear(emb_size, n_classes))"],"metadata":{"id":"w0HUIwa_5ypX","executionInfo":{"status":"ok","timestamp":1642085167227,"user_tz":-540,"elapsed":282,"user":{"displayName":"ᄋᄋᄌ","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07872107978641370856"}}},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":["## **Summary**"],"metadata":{"id":"IzsNcH7C6r8o"}},{"cell_type":"code","source":["class ViT(nn.Sequential):\n","    def __init__(self,     \n","                in_channels: int = 3,\n","                patch_size: int = 16,\n","                emb_size: int = 768,\n","                img_size: int = 224,\n","                depth: int = 12,\n","                n_classes: int = 1000,\n","                **kwargs):\n","        super().__init__(\n","            PatchEmbedding(in_channels, patch_size, emb_size, img_size),\n","            TransformerEncoder(depth, emb_size=emb_size, **kwargs),\n","            ClassificationHead(emb_size, n_classes)\n","        )"],"metadata":{"id":"dkZz4CAJ6tbA","executionInfo":{"status":"ok","timestamp":1642085414515,"user_tz":-540,"elapsed":299,"user":{"displayName":"ᄋᄋᄌ","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07872107978641370856"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["summary(ViT(), (3, 224, 224), device='cpu')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-VU0I4x66usy","executionInfo":{"status":"ok","timestamp":1642085424956,"user_tz":-540,"elapsed":2372,"user":{"displayName":"ᄋᄋᄌ","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07872107978641370856"}},"outputId":"25115235-adec-499a-ad7f-af3b6257c70d"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1          [-1, 768, 14, 14]         590,592\n","         Rearrange-2             [-1, 196, 768]               0\n","    PatchEmbedding-3             [-1, 197, 768]               0\n","         LayerNorm-4             [-1, 197, 768]           1,536\n","            Linear-5            [-1, 197, 2304]       1,771,776\n","           Dropout-6          [-1, 8, 197, 197]               0\n","            Linear-7             [-1, 197, 768]         590,592\n","MultiHeadAttention-8             [-1, 197, 768]               0\n","           Dropout-9             [-1, 197, 768]               0\n","      ResidualAdd-10             [-1, 197, 768]               0\n","        LayerNorm-11             [-1, 197, 768]           1,536\n","           Linear-12            [-1, 197, 3072]       2,362,368\n","             GELU-13            [-1, 197, 3072]               0\n","          Dropout-14            [-1, 197, 3072]               0\n","           Linear-15             [-1, 197, 768]       2,360,064\n","          Dropout-16             [-1, 197, 768]               0\n","      ResidualAdd-17             [-1, 197, 768]               0\n","        LayerNorm-18             [-1, 197, 768]           1,536\n","           Linear-19            [-1, 197, 2304]       1,771,776\n","          Dropout-20          [-1, 8, 197, 197]               0\n","           Linear-21             [-1, 197, 768]         590,592\n","MultiHeadAttention-22             [-1, 197, 768]               0\n","          Dropout-23             [-1, 197, 768]               0\n","      ResidualAdd-24             [-1, 197, 768]               0\n","        LayerNorm-25             [-1, 197, 768]           1,536\n","           Linear-26            [-1, 197, 3072]       2,362,368\n","             GELU-27            [-1, 197, 3072]               0\n","          Dropout-28            [-1, 197, 3072]               0\n","           Linear-29             [-1, 197, 768]       2,360,064\n","          Dropout-30             [-1, 197, 768]               0\n","      ResidualAdd-31             [-1, 197, 768]               0\n","        LayerNorm-32             [-1, 197, 768]           1,536\n","           Linear-33            [-1, 197, 2304]       1,771,776\n","          Dropout-34          [-1, 8, 197, 197]               0\n","           Linear-35             [-1, 197, 768]         590,592\n","MultiHeadAttention-36             [-1, 197, 768]               0\n","          Dropout-37             [-1, 197, 768]               0\n","      ResidualAdd-38             [-1, 197, 768]               0\n","        LayerNorm-39             [-1, 197, 768]           1,536\n","           Linear-40            [-1, 197, 3072]       2,362,368\n","             GELU-41            [-1, 197, 3072]               0\n","          Dropout-42            [-1, 197, 3072]               0\n","           Linear-43             [-1, 197, 768]       2,360,064\n","          Dropout-44             [-1, 197, 768]               0\n","      ResidualAdd-45             [-1, 197, 768]               0\n","        LayerNorm-46             [-1, 197, 768]           1,536\n","           Linear-47            [-1, 197, 2304]       1,771,776\n","          Dropout-48          [-1, 8, 197, 197]               0\n","           Linear-49             [-1, 197, 768]         590,592\n","MultiHeadAttention-50             [-1, 197, 768]               0\n","          Dropout-51             [-1, 197, 768]               0\n","      ResidualAdd-52             [-1, 197, 768]               0\n","        LayerNorm-53             [-1, 197, 768]           1,536\n","           Linear-54            [-1, 197, 3072]       2,362,368\n","             GELU-55            [-1, 197, 3072]               0\n","          Dropout-56            [-1, 197, 3072]               0\n","           Linear-57             [-1, 197, 768]       2,360,064\n","          Dropout-58             [-1, 197, 768]               0\n","      ResidualAdd-59             [-1, 197, 768]               0\n","        LayerNorm-60             [-1, 197, 768]           1,536\n","           Linear-61            [-1, 197, 2304]       1,771,776\n","          Dropout-62          [-1, 8, 197, 197]               0\n","           Linear-63             [-1, 197, 768]         590,592\n","MultiHeadAttention-64             [-1, 197, 768]               0\n","          Dropout-65             [-1, 197, 768]               0\n","      ResidualAdd-66             [-1, 197, 768]               0\n","        LayerNorm-67             [-1, 197, 768]           1,536\n","           Linear-68            [-1, 197, 3072]       2,362,368\n","             GELU-69            [-1, 197, 3072]               0\n","          Dropout-70            [-1, 197, 3072]               0\n","           Linear-71             [-1, 197, 768]       2,360,064\n","          Dropout-72             [-1, 197, 768]               0\n","      ResidualAdd-73             [-1, 197, 768]               0\n","        LayerNorm-74             [-1, 197, 768]           1,536\n","           Linear-75            [-1, 197, 2304]       1,771,776\n","          Dropout-76          [-1, 8, 197, 197]               0\n","           Linear-77             [-1, 197, 768]         590,592\n","MultiHeadAttention-78             [-1, 197, 768]               0\n","          Dropout-79             [-1, 197, 768]               0\n","      ResidualAdd-80             [-1, 197, 768]               0\n","        LayerNorm-81             [-1, 197, 768]           1,536\n","           Linear-82            [-1, 197, 3072]       2,362,368\n","             GELU-83            [-1, 197, 3072]               0\n","          Dropout-84            [-1, 197, 3072]               0\n","           Linear-85             [-1, 197, 768]       2,360,064\n","          Dropout-86             [-1, 197, 768]               0\n","      ResidualAdd-87             [-1, 197, 768]               0\n","        LayerNorm-88             [-1, 197, 768]           1,536\n","           Linear-89            [-1, 197, 2304]       1,771,776\n","          Dropout-90          [-1, 8, 197, 197]               0\n","           Linear-91             [-1, 197, 768]         590,592\n","MultiHeadAttention-92             [-1, 197, 768]               0\n","          Dropout-93             [-1, 197, 768]               0\n","      ResidualAdd-94             [-1, 197, 768]               0\n","        LayerNorm-95             [-1, 197, 768]           1,536\n","           Linear-96            [-1, 197, 3072]       2,362,368\n","             GELU-97            [-1, 197, 3072]               0\n","          Dropout-98            [-1, 197, 3072]               0\n","           Linear-99             [-1, 197, 768]       2,360,064\n","         Dropout-100             [-1, 197, 768]               0\n","     ResidualAdd-101             [-1, 197, 768]               0\n","       LayerNorm-102             [-1, 197, 768]           1,536\n","          Linear-103            [-1, 197, 2304]       1,771,776\n","         Dropout-104          [-1, 8, 197, 197]               0\n","          Linear-105             [-1, 197, 768]         590,592\n","MultiHeadAttention-106             [-1, 197, 768]               0\n","         Dropout-107             [-1, 197, 768]               0\n","     ResidualAdd-108             [-1, 197, 768]               0\n","       LayerNorm-109             [-1, 197, 768]           1,536\n","          Linear-110            [-1, 197, 3072]       2,362,368\n","            GELU-111            [-1, 197, 3072]               0\n","         Dropout-112            [-1, 197, 3072]               0\n","          Linear-113             [-1, 197, 768]       2,360,064\n","         Dropout-114             [-1, 197, 768]               0\n","     ResidualAdd-115             [-1, 197, 768]               0\n","       LayerNorm-116             [-1, 197, 768]           1,536\n","          Linear-117            [-1, 197, 2304]       1,771,776\n","         Dropout-118          [-1, 8, 197, 197]               0\n","          Linear-119             [-1, 197, 768]         590,592\n","MultiHeadAttention-120             [-1, 197, 768]               0\n","         Dropout-121             [-1, 197, 768]               0\n","     ResidualAdd-122             [-1, 197, 768]               0\n","       LayerNorm-123             [-1, 197, 768]           1,536\n","          Linear-124            [-1, 197, 3072]       2,362,368\n","            GELU-125            [-1, 197, 3072]               0\n","         Dropout-126            [-1, 197, 3072]               0\n","          Linear-127             [-1, 197, 768]       2,360,064\n","         Dropout-128             [-1, 197, 768]               0\n","     ResidualAdd-129             [-1, 197, 768]               0\n","       LayerNorm-130             [-1, 197, 768]           1,536\n","          Linear-131            [-1, 197, 2304]       1,771,776\n","         Dropout-132          [-1, 8, 197, 197]               0\n","          Linear-133             [-1, 197, 768]         590,592\n","MultiHeadAttention-134             [-1, 197, 768]               0\n","         Dropout-135             [-1, 197, 768]               0\n","     ResidualAdd-136             [-1, 197, 768]               0\n","       LayerNorm-137             [-1, 197, 768]           1,536\n","          Linear-138            [-1, 197, 3072]       2,362,368\n","            GELU-139            [-1, 197, 3072]               0\n","         Dropout-140            [-1, 197, 3072]               0\n","          Linear-141             [-1, 197, 768]       2,360,064\n","         Dropout-142             [-1, 197, 768]               0\n","     ResidualAdd-143             [-1, 197, 768]               0\n","       LayerNorm-144             [-1, 197, 768]           1,536\n","          Linear-145            [-1, 197, 2304]       1,771,776\n","         Dropout-146          [-1, 8, 197, 197]               0\n","          Linear-147             [-1, 197, 768]         590,592\n","MultiHeadAttention-148             [-1, 197, 768]               0\n","         Dropout-149             [-1, 197, 768]               0\n","     ResidualAdd-150             [-1, 197, 768]               0\n","       LayerNorm-151             [-1, 197, 768]           1,536\n","          Linear-152            [-1, 197, 3072]       2,362,368\n","            GELU-153            [-1, 197, 3072]               0\n","         Dropout-154            [-1, 197, 3072]               0\n","          Linear-155             [-1, 197, 768]       2,360,064\n","         Dropout-156             [-1, 197, 768]               0\n","     ResidualAdd-157             [-1, 197, 768]               0\n","       LayerNorm-158             [-1, 197, 768]           1,536\n","          Linear-159            [-1, 197, 2304]       1,771,776\n","         Dropout-160          [-1, 8, 197, 197]               0\n","          Linear-161             [-1, 197, 768]         590,592\n","MultiHeadAttention-162             [-1, 197, 768]               0\n","         Dropout-163             [-1, 197, 768]               0\n","     ResidualAdd-164             [-1, 197, 768]               0\n","       LayerNorm-165             [-1, 197, 768]           1,536\n","          Linear-166            [-1, 197, 3072]       2,362,368\n","            GELU-167            [-1, 197, 3072]               0\n","         Dropout-168            [-1, 197, 3072]               0\n","          Linear-169             [-1, 197, 768]       2,360,064\n","         Dropout-170             [-1, 197, 768]               0\n","     ResidualAdd-171             [-1, 197, 768]               0\n","          Reduce-172                  [-1, 768]               0\n","       LayerNorm-173                  [-1, 768]           1,536\n","          Linear-174                 [-1, 1000]         769,000\n","================================================================\n","Total params: 86,415,592\n","Trainable params: 86,415,592\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.57\n","Forward/backward pass size (MB): 364.33\n","Params size (MB): 329.65\n","Estimated Total Size (MB): 694.56\n","----------------------------------------------------------------\n"]}]}]}