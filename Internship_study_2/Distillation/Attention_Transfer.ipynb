{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Attention_Transfer.ipynb","provenance":[],"collapsed_sections":["giFiXOGannGr","KaPVhbyNvRrR"],"authorship_tag":"ABX9TyOfZr48qia1qReaqt6ZZR11"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","참고\n","> https://github.com/HobbitLong/RepDistiller/tree/dcc043277f2820efafd679ffb82b8e8195b7e222\n","\n","> https://sytoday.tistory.com/3\n","\n","\n","\n"],"metadata":{"id":"QYyVgCQgzsIN"}},{"cell_type":"markdown","source":["# **Model**"],"metadata":{"id":"uJ2lBgU-I8Qb"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"yDaOBKgTIral","executionInfo":{"status":"ok","timestamp":1646484811292,"user_tz":-540,"elapsed":8051,"user":{"displayName":"‍이의준[학생](소프트웨어융합대학 컴퓨터공학부)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06208559590960942181"}}},"outputs":[],"source":["from __future__ import absolute_import\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math\n","\n","\n","__all__ = ['resnet']\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    \"\"\"3x3 convolution with padding\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n","                     padding=1, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None, is_last=False):\n","        super(BasicBlock, self).__init__()\n","        self.is_last = is_last\n","        self.conv1 = conv3x3(inplanes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x):\n","        residual = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out += residual\n","        preact = out\n","        out = F.relu(out)\n","        if self.is_last:\n","            return out, preact\n","        else:\n","            return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None, is_last=False):\n","        super(Bottleneck, self).__init__()\n","        self.is_last = is_last\n","        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n","                               padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes * 4)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x):\n","        residual = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = self.relu(out)\n","\n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out += residual\n","        preact = out\n","        out = F.relu(out)\n","        if self.is_last:\n","            return out, preact\n","        else:\n","            return out\n","\n","\n","class ResNet(nn.Module):\n","\n","    def __init__(self, depth, num_filters, block_name='BasicBlock', num_classes=10):\n","        super(ResNet, self).__init__()\n","        # Model type specifies number of layers for CIFAR-10 model\n","        if block_name.lower() == 'basicblock':\n","            assert (depth - 2) % 6 == 0, 'When use basicblock, depth should be 6n+2, e.g. 20, 32, 44, 56, 110, 1202'\n","            n = (depth - 2) // 6\n","            block = BasicBlock\n","        elif block_name.lower() == 'bottleneck':\n","            assert (depth - 2) % 9 == 0, 'When use bottleneck, depth should be 9n+2, e.g. 20, 29, 47, 56, 110, 1199'\n","            n = (depth - 2) // 9\n","            block = Bottleneck\n","        else:\n","            raise ValueError('block_name shoule be Basicblock or Bottleneck')\n","\n","        self.inplanes = num_filters[0]\n","        self.conv1 = nn.Conv2d(3, num_filters[0], kernel_size=3, padding=1,\n","                               bias=False)\n","        self.bn1 = nn.BatchNorm2d(num_filters[0])\n","        self.relu = nn.ReLU(inplace=True)\n","        self.layer1 = self._make_layer(block, num_filters[1], n)\n","        self.layer2 = self._make_layer(block, num_filters[2], n, stride=2)\n","        self.layer3 = self._make_layer(block, num_filters[3], n, stride=2)\n","        self.avgpool = nn.AvgPool2d(8)\n","        self.fc = nn.Linear(num_filters[3] * block.expansion, num_classes)\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","\n","    def _make_layer(self, block, planes, blocks, stride=1):\n","        downsample = None\n","        if stride != 1 or self.inplanes != planes * block.expansion:\n","            downsample = nn.Sequential(\n","                nn.Conv2d(self.inplanes, planes * block.expansion,\n","                          kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(planes * block.expansion),\n","            )\n","\n","        layers = list([])\n","        layers.append(block(self.inplanes, planes, stride, downsample, is_last=(blocks == 1)))\n","        self.inplanes = planes * block.expansion\n","        for i in range(1, blocks):\n","            layers.append(block(self.inplanes, planes, is_last=(i == blocks-1)))\n","\n","        return nn.Sequential(*layers)\n","\n","    def get_feat_modules(self):\n","        feat_m = nn.ModuleList([])\n","        feat_m.append(self.conv1)\n","        feat_m.append(self.bn1)\n","        feat_m.append(self.relu)\n","        feat_m.append(self.layer1)\n","        feat_m.append(self.layer2)\n","        feat_m.append(self.layer3)\n","        return feat_m\n","\n","    def get_bn_before_relu(self):\n","        if isinstance(self.layer1[0], Bottleneck):\n","            bn1 = self.layer1[-1].bn3\n","            bn2 = self.layer2[-1].bn3\n","            bn3 = self.layer3[-1].bn3\n","        elif isinstance(self.layer1[0], BasicBlock):\n","            bn1 = self.layer1[-1].bn2\n","            bn2 = self.layer2[-1].bn2\n","            bn3 = self.layer3[-1].bn2\n","        else:\n","            raise NotImplementedError('ResNet unknown block error !!!')\n","\n","        return [bn1, bn2, bn3]\n","\n","    def forward(self, x, is_feat=False, preact=False):\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)  # 32x32\n","        f0 = x\n","\n","        x, f1_pre = self.layer1(x)  # 32x32\n","        f1 = x\n","        x, f2_pre = self.layer2(x)  # 16x16\n","        f2 = x\n","        x, f3_pre = self.layer3(x)  # 8x8\n","        f3 = x\n","\n","        x = self.avgpool(x)\n","        x = x.view(x.size(0), -1)\n","        f4 = x\n","        x = self.fc(x)\n","\n","        if is_feat:\n","            if preact:\n","                return [f0, f1_pre, f2_pre, f3_pre, f4], x\n","            else:\n","                return [f0, f1, f2, f3, f4], x\n","        else:\n","            return x\n","\n","\n","def resnet8(**kwargs):\n","    return ResNet(8, [16, 16, 32, 64], 'basicblock', **kwargs)\n","\n","\n","def resnet14(**kwargs):\n","    return ResNet(14, [16, 16, 32, 64], 'basicblock', **kwargs)\n","\n","\n","def resnet20(**kwargs):\n","    return ResNet(20, [16, 16, 32, 64], 'basicblock', **kwargs)\n","\n","\n","def resnet32(**kwargs):\n","    return ResNet(32, [16, 16, 32, 64], 'basicblock', **kwargs)\n","\n","\n","def resnet44(**kwargs):\n","    return ResNet(44, [16, 16, 32, 64], 'basicblock', **kwargs)\n","\n","\n","def resnet56(**kwargs):\n","    return ResNet(56, [16, 16, 32, 64], 'basicblock', **kwargs)\n","\n","\n","def resnet110(**kwargs):\n","    return ResNet(110, [16, 16, 32, 64], 'basicblock', **kwargs)\n","\n","\n","def resnet8x4(**kwargs):\n","    return ResNet(8, [32, 64, 128, 256], 'basicblock', **kwargs)\n","\n","\n","def resnet32x4(**kwargs):\n","    return ResNet(32, [32, 64, 128, 256], 'basicblock', **kwargs)"]},{"cell_type":"markdown","source":["# **Helper**"],"metadata":{"id":"T-SQk6whjoA6"}},{"cell_type":"code","source":["from __future__ import print_function, division\n","\n","import sys\n","import time\n","import torch\n","\n","\n","#from .util import AverageMeter, accuracy   아래 구현된 코드임\n","class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","def accuracy(output, target, topk=(1,)):\n","    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n","    with torch.no_grad():\n","        maxk = max(topk)\n","        batch_size = target.size(0)\n","\n","        _, pred = output.topk(maxk, 1, True, True)\n","        pred = pred.t()\n","        correct = pred.eq(target.view(1, -1).expand_as(pred))\n","\n","        res = []\n","        for k in topk:\n","            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n","            res.append(correct_k.mul_(100.0 / batch_size))\n","        return res"],"metadata":{"id":"Wn3Z5QQHjs9d","executionInfo":{"status":"ok","timestamp":1646494008024,"user_tz":-540,"elapsed":336,"user":{"displayName":"‍이의준[학생](소프트웨어융합대학 컴퓨터공학부)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06208559590960942181"}}},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":["## **for training teacher model**"],"metadata":{"id":"GaxzCCtkkOjP"}},{"cell_type":"code","source":["def train_vanilla(epoch, train_loader, model, criterion, optimizer, opt):\n","    \"\"\"vanilla training\"\"\"\n","    model.train()\n","\t\n","    #초기화\n","    batch_time = AverageMeter()\n","    data_time = AverageMeter()\n","    losses = AverageMeter()\n","    top1 = AverageMeter()\n","    top5 = AverageMeter()\n","\n","    end = time.time()\n","    for idx, (input, target) in enumerate(train_loader):\n","        data_time.update(time.time() - end)\n","\n","        input = input.float()\n","        if torch.cuda.is_available():\n","            input = input.cuda()\n","            target = target.cuda()\n","\n","        # ===================forward=====================\n","        output = model(input)\n","        loss = criterion(output, target)\n","\n","        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n","        losses.update(loss.item(), input.size(0))\n","        top1.update(acc1[0], input.size(0))\n","        top5.update(acc5[0], input.size(0))\n","\n","        # ===================backward=====================\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # ===================meters=====================\n","        batch_time.update(time.time() - end)\n","        end = time.time()\n","\n","        # tensorboard logger\n","        pass\n","\n","        # print info\n","        if idx % opt.print_freq == 0:\n","            print('Epoch: [{0}][{1}/{2}]\\t'\n","                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n","                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n","                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n","                  'Acc@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n","                  'Acc@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n","                   epoch, idx, len(train_loader), batch_time=batch_time,\n","                   data_time=data_time, loss=losses, top1=top1, top5=top5))\n","            sys.stdout.flush()\n","\n","    print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n","          .format(top1=top1, top5=top5))\n","\n","    return top1.avg, losses.avg"],"metadata":{"id":"EPnMWqH8kS_0","executionInfo":{"status":"ok","timestamp":1646491941395,"user_tz":-540,"elapsed":442,"user":{"displayName":"‍이의준[학생](소프트웨어융합대학 컴퓨터공학부)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06208559590960942181"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## **for training student model**"],"metadata":{"id":"jY2VvrHnkhPd"}},{"cell_type":"code","source":["def train_distill(epoch, train_loader, module_list, criterion_list, optimizer, opt):\n","    \"\"\"One epoch distillation\"\"\"\n","    # set modules as train()\n","    for module in module_list:\n","        module.train()\n","    # set teacher as eval() -> main()에서 teacher network가 list의 맨 마지막에 가도록 설정\n","    module_list[-1].eval()\n","\n","    criterion_cls = criterion_list[0]\n","    criterion_div = criterion_list[1]\n","    criterion_kd = criterion_list[2]\n","\n","    model_s = module_list[0]\n","    model_t = module_list[-1]\n","\n","    batch_time = AverageMeter()\n","    data_time = AverageMeter()\n","    losses = AverageMeter()\n","    top1 = AverageMeter()\n","    top5 = AverageMeter()\n","\n","    end = time.time()\n","    for idx, data in enumerate(train_loader):\n","        input, target, index = data\n","        data_time.update(time.time() - end)\n","\n","        input = input.float()\n","        if torch.cuda.is_available():\n","            input = input.cuda()\n","            target = target.cuda()\n","            index = index.cuda()\n","        # ===================forward=====================\n","        preact = False\n","        #knowledge distiilation 방식이 'abound'인 경우에 실행\n","        if opt.distill in ['abound']:\n","            preact = True\n","            \n","        #models resnet.py에서 보면 is_feat=True일 경우, feat와 output을 return\n","        #return 값 : [f0,f1,f2,f3,f4],x\n","        feat_s, logit_s = model_s(input, is_feat=True, preact=preact)\n","        with torch.no_grad():\n","            feat_t, logit_t = model_t(input, is_feat=True, preact=preact)\n","            \n","            #teache network의 feat_t의 경우 detach()함수를 통해 연산이 추적되는 것을 방지\n","            feat_t = [f.detach() for f in feat_t]\n","\n","        # loss_cls : cross entropy loss // loss_div : KL divergence loss\n","        # student loss function : student network의 prediction과 ground-truth의 차이\n","        # distillation loss function : soft student predictions과 soft teacher labels의 차이\n","        loss_cls = criterion_cls(logit_s, target)\n","        loss_div = criterion_div(logit_s, logit_t)\n","        \n","        g_s = feat_s[1:-1]\n","        g_t = feat_t[1:-1]\n","        loss_group = criterion_kd(g_s, g_t)\n","        loss_kd = sum(loss_group)\n","        \n","\t\t# 여기의 loss는 다양한 knowledge distillation을 통합해서 나타낼 수 있도록 이렇게 작성\n","        # Attention Transfer의 경우 loss_cls와 loss_kd(AT)로 구성되어 있음\n","        # 4.1 cifar experiments : argument를 설정할 때 alpha 값을 0.9으로 설정, KD_T(temperature)값은 4로 설정\n","        # gamma와 beta값은 논문 상의 값으로 설정\n","        loss = opt.gamma * loss_cls + opt.alpha * loss_div + opt.beta * loss_kd\n","\n","        acc1, acc5 = accuracy(logit_s, target, topk=(1, 5))\n","        losses.update(loss.item(), input.size(0))\n","        top1.update(acc1[0], input.size(0))\n","        top5.update(acc5[0], input.size(0))\n","\n","        # ===================backward=====================\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # ===================meters=====================\n","        batch_time.update(time.time() - end)\n","        end = time.time()\n","\n","        # print info\n","        if idx % opt.print_freq == 0:\n","            print('Epoch: [{0}][{1}/{2}]\\t'\n","                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n","                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n","                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n","                  'Acc@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n","                  'Acc@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n","                epoch, idx, len(train_loader), batch_time=batch_time,\n","                data_time=data_time, loss=losses, top1=top1, top5=top5))\n","            sys.stdout.flush()\n","\n","    print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n","          .format(top1=top1, top5=top5))\n","\n","    return top1.avg, losses.avg"],"metadata":{"id":"YiuJiynJklgi","executionInfo":{"status":"ok","timestamp":1646492017938,"user_tz":-540,"elapsed":438,"user":{"displayName":"‍이의준[학생](소프트웨어융합대학 컴퓨터공학부)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06208559590960942181"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## **for validation**"],"metadata":{"id":"Czsqo-fskpl-"}},{"cell_type":"code","source":["def validate(val_loader, model, criterion, opt):\n","    \"\"\"validation\"\"\"\n","    batch_time = AverageMeter()\n","    losses = AverageMeter()\n","    top1 = AverageMeter()\n","    top5 = AverageMeter()\n","\n","    # switch to evaluate mode\n","    model.eval()\n","\n","    with torch.no_grad():\n","        end = time.time()\n","        for idx, (input, target) in enumerate(val_loader):\n","\n","            input = input.float()\n","            if torch.cuda.is_available():\n","                input = input.cuda()\n","                target = target.cuda()\n","\n","            # compute output\n","            output = model(input)\n","            loss = criterion(output, target)\n","\n","            # measure accuracy and record loss\n","            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n","            losses.update(loss.item(), input.size(0))\n","            top1.update(acc1[0], input.size(0))\n","            top5.update(acc5[0], input.size(0))\n","\n","            # measure elapsed time\n","            batch_time.update(time.time() - end)\n","            end = time.time()\n","\n","            if idx % opt.print_freq == 0:\n","                print('Test: [{0}/{1}]\\t'\n","                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n","                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n","                      'Acc@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n","                      'Acc@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n","                       idx, len(val_loader), batch_time=batch_time, loss=losses,\n","                       top1=top1, top5=top5))\n","\n","        print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n","              .format(top1=top1, top5=top5))\n","\n","    return top1.avg, top5.avg, losses.avg"],"metadata":{"id":"E_mlap21ko_G","executionInfo":{"status":"ok","timestamp":1646492345764,"user_tz":-540,"elapsed":348,"user":{"displayName":"‍이의준[학생](소프트웨어융합대학 컴퓨터공학부)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06208559590960942181"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["## **utils**"],"metadata":{"id":"HG1BvhDXn7NG"}},{"cell_type":"code","source":["from __future__ import print_function\n","\n","import torch\n","import numpy as np\n","\n","\n","def adjust_learning_rate(epoch, opt, optimizer):\n","    \"\"\"Sets the learning rate to the initial LR decayed by decay rate every steep step\"\"\"\n","    steps = np.sum(epoch > np.asarray(opt.lr_decay_epochs))\n","    if steps > 0:\n","        new_lr = opt.learning_rate * (opt.lr_decay_rate ** steps)\n","        for param_group in optimizer.param_groups:\n","            param_group['lr'] = new_lr\n","\n","\n","class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","def accuracy(output, target, topk=(1,)):\n","    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n","    with torch.no_grad():\n","        maxk = max(topk)\n","        batch_size = target.size(0)\n","\n","        _, pred = output.topk(maxk, 1, True, True)\n","        pred = pred.t()\n","        correct = pred.eq(target.view(1, -1).expand_as(pred))\n","\n","        res = []\n","        for k in topk:\n","            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n","            res.append(correct_k.mul_(100.0 / batch_size))\n","        return res\n","\n","\n","if __name__ == '__main__':\n","\n","    pass"],"metadata":{"id":"lovwRNvjn9vi","executionInfo":{"status":"ok","timestamp":1646492915407,"user_tz":-540,"elapsed":404,"user":{"displayName":"‍이의준[학생](소프트웨어융합대학 컴퓨터공학부)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06208559590960942181"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["# **Training Teacher**"],"metadata":{"id":"TNCO8njlmJi1"}},{"cell_type":"markdown","source":["## **import**"],"metadata":{"id":"QwRRmyO1ngu5"}},{"cell_type":"code","source":["!pip install tensorboard_logger"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sCXRqwwhmhTi","executionInfo":{"status":"ok","timestamp":1646492535484,"user_tz":-540,"elapsed":4549,"user":{"displayName":"‍이의준[학생](소프트웨어융합대학 컴퓨터공학부)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06208559590960942181"}},"outputId":"1dcb8705-a6cf-4e2b-a6e9-184fefefdebe"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorboard_logger\n","  Downloading tensorboard_logger-0.1.0-py2.py3-none-any.whl (17 kB)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from tensorboard_logger) (3.17.3)\n","Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard_logger) (1.4.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboard_logger) (1.15.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboard_logger) (1.21.5)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard_logger) (7.1.2)\n","Installing collected packages: tensorboard-logger\n","Successfully installed tensorboard-logger-0.1.0\n"]}]},{"cell_type":"code","source":["from __future__ import print_function\n","\n","import os\n","import argparse\n","import socket\n","import time\n","\n","import tensorboard_logger as tb_logger\n","import torch\n","import torch.optim as optim\n","import torch.nn as nn\n","import torch.backends.cudnn as cudnn\n","\n","#from models import model_dict\n","\n","model_dict = {\n","    'resnet8': resnet8,\n","    'resnet14': resnet14,\n","    'resnet20': resnet20,\n","    'resnet32': resnet32,\n","    'resnet44': resnet44,\n","    'resnet56': resnet56,\n","    'resnet110': resnet110,\n","    'resnet8x4': resnet8x4,\n","    'resnet32x4': resnet32x4,\n","}\n","\n","#from dataset.cifar100 import get_cifar100_dataloaders\n","\n","#from helper.util import adjust_learning_rate, accuracy, AverageMeter\n","#from helper.loops import train_vanilla as train, validate\n","\n","import ssl\n","ssl._create_default_https_context = ssl._create_unverified_context\n","cudnn.benchmark = True"],"metadata":{"id":"BCyaNi_BmRxQ","executionInfo":{"status":"ok","timestamp":1646492955863,"user_tz":-540,"elapsed":332,"user":{"displayName":"‍이의준[학생](소프트웨어융합대학 컴퓨터공학부)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06208559590960942181"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["### from dataset.cifar100 import get_cifar100_dataloaders"],"metadata":{"id":"giFiXOGannGr"}},{"cell_type":"code","source":["from __future__ import print_function\n","\n","import os\n","import socket\n","import numpy as np\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","from PIL import Image\n","\n","\"\"\"\n","mean = {\n","    'cifar100': (0.5071, 0.4867, 0.4408),\n","}\n","std = {\n","    'cifar100': (0.2675, 0.2565, 0.2761),\n","}\n","\"\"\"\n","\n","\n","def get_data_folder():\n","    \"\"\"\n","    return server-dependent path to store the data\n","    \"\"\"\n","    hostname = socket.gethostname()\n","    if hostname.startswith('visiongpu'):\n","        data_folder = '/data/vision/phillipi/rep-learn/datasets'\n","    elif hostname.startswith('yonglong-home'):\n","        data_folder = '/home/yonglong/Data/data'\n","    else:\n","        data_folder = './data/'\n","\n","    if not os.path.isdir(data_folder):\n","        os.makedirs(data_folder)\n","\n","    return data_folder\n","\n","\n","class CIFAR100Instance(datasets.CIFAR100):\n","    \"\"\"CIFAR100Instance Dataset.\n","    \"\"\"\n","    def __getitem__(self, index):\n","        if self.train:\n","            img, target = self.train_data[index], self.train_labels[index]\n","        else:\n","            img, target = self.test_data[index], self.test_labels[index]\n","\n","        # doing this so that it is consistent with all other datasets\n","        # to return a PIL Image\n","        img = Image.fromarray(img)\n","\n","        if self.transform is not None:\n","            img = self.transform(img)\n","\n","        if self.target_transform is not None:\n","            target = self.target_transform(target)\n","\n","        return img, target, index\n","\n","\n","def get_cifar100_dataloaders(batch_size=128, num_workers=8, is_instance=False):\n","    \"\"\"\n","    cifar 100\n","    \"\"\"\n","    data_folder = get_data_folder()\n","\n","    train_transform = transforms.Compose([\n","        transforms.RandomCrop(32, padding=4),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n","    ])\n","    test_transform = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n","    ])\n","\n","    if is_instance:\n","        train_set = CIFAR100Instance(root=data_folder,\n","                                     download=True,\n","                                     train=True,\n","                                     transform=train_transform)\n","        n_data = len(train_set)\n","    else:\n","        train_set = datasets.CIFAR100(root=data_folder,\n","                                      download=True,\n","                                      train=True,\n","                                      transform=train_transform)\n","    train_loader = DataLoader(train_set,\n","                              batch_size=batch_size,\n","                              shuffle=True,\n","                              num_workers=num_workers)\n","\n","    test_set = datasets.CIFAR100(root=data_folder,\n","                                 download=True,\n","                                 train=False,\n","                                 transform=test_transform)\n","    test_loader = DataLoader(test_set,\n","                             batch_size=int(batch_size/2),\n","                             shuffle=False,\n","                             num_workers=int(num_workers/2))\n","\n","    if is_instance:\n","        return train_loader, test_loader, n_data\n","    else:\n","        return train_loader, test_loader\n","\n","class CIFAR100InstanceSample(datasets.CIFAR100):\n","    \"\"\"\n","    CIFAR100Instance+Sample Dataset\n","    \"\"\"\n","    def __init__(self, root, train=True,\n","                 transform=None, target_transform=None,\n","                 download=False, k=4096, mode='exact', is_sample=True, percent=1.0):\n","        super().__init__(root=root, train=train, download=download,\n","                         transform=transform, target_transform=target_transform)\n","        self.k = k\n","        self.mode = mode\n","        self.is_sample = is_sample\n","\n","        num_classes = 100\n","        if self.train:\n","            num_samples = len(self.train_data)\n","            label = self.train_labels\n","        else:\n","            num_samples = len(self.test_data)\n","            label = self.test_labels\n","\n","        self.cls_positive = [[] for i in range(num_classes)]\n","        for i in range(num_samples):\n","            self.cls_positive[label[i]].append(i)\n","\n","        self.cls_negative = [[] for i in range(num_classes)]\n","        for i in range(num_classes):\n","            for j in range(num_classes):\n","                if j == i:\n","                    continue\n","                self.cls_negative[i].extend(self.cls_positive[j])\n","\n","        self.cls_positive = [np.asarray(self.cls_positive[i]) for i in range(num_classes)]\n","        self.cls_negative = [np.asarray(self.cls_negative[i]) for i in range(num_classes)]\n","\n","        if 0 < percent < 1:\n","            n = int(len(self.cls_negative[0]) * percent)\n","            self.cls_negative = [np.random.permutation(self.cls_negative[i])[0:n]\n","                                 for i in range(num_classes)]\n","\n","        self.cls_positive = np.asarray(self.cls_positive)\n","        self.cls_negative = np.asarray(self.cls_negative)\n","\n","    def __getitem__(self, index):\n","        if self.train:\n","            img, target = self.train_data[index], self.train_labels[index]\n","        else:\n","            img, target = self.test_data[index], self.test_labels[index]\n","\n","        # doing this so that it is consistent with all other datasets\n","        # to return a PIL Image\n","        img = Image.fromarray(img)\n","\n","        if self.transform is not None:\n","            img = self.transform(img)\n","\n","        if self.target_transform is not None:\n","            target = self.target_transform(target)\n","\n","        if not self.is_sample:\n","            # directly return\n","            return img, target, index\n","        else:\n","            # sample contrastive examples\n","            if self.mode == 'exact':\n","                pos_idx = index\n","            elif self.mode == 'relax':\n","                pos_idx = np.random.choice(self.cls_positive[target], 1)\n","                pos_idx = pos_idx[0]\n","            else:\n","                raise NotImplementedError(self.mode)\n","            replace = True if self.k > len(self.cls_negative[target]) else False\n","            neg_idx = np.random.choice(self.cls_negative[target], self.k, replace=replace)\n","            sample_idx = np.hstack((np.asarray([pos_idx]), neg_idx))\n","            return img, target, index, sample_idx\n","\n","\n","def get_cifar100_dataloaders_sample(batch_size=128, num_workers=8, k=4096, mode='exact',\n","                                    is_sample=True, percent=1.0):\n","    \"\"\"\n","    cifar 100\n","    \"\"\"\n","    data_folder = get_data_folder()\n","\n","    train_transform = transforms.Compose([\n","        transforms.RandomCrop(32, padding=4),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n","    ])\n","    test_transform = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n","    ])\n","\n","    train_set = CIFAR100InstanceSample(root=data_folder,\n","                                       download=True,\n","                                       train=True,\n","                                       transform=train_transform,\n","                                       k=k,\n","                                       mode=mode,\n","                                       is_sample=is_sample,\n","                                       percent=percent)\n","    n_data = len(train_set)\n","    train_loader = DataLoader(train_set,\n","                              batch_size=batch_size,\n","                              shuffle=True,\n","                              num_workers=num_workers)\n","\n","    test_set = datasets.CIFAR100(root=data_folder,\n","                                 download=True,\n","                                 train=False,\n","                                 transform=test_transform)\n","    test_loader = DataLoader(test_set,\n","                             batch_size=int(batch_size/2),\n","                             shuffle=False,\n","                             num_workers=int(num_workers/2))\n","\n","    return train_loader, test_loader, n_data"],"metadata":{"id":"B-LJ2_PSnkPt","executionInfo":{"status":"ok","timestamp":1646494842514,"user_tz":-540,"elapsed":338,"user":{"displayName":"‍이의준[학생](소프트웨어융합대학 컴퓨터공학부)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06208559590960942181"}}},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":["## **set arguments**"],"metadata":{"id":"J2D0pNtroZ51"}},{"cell_type":"code","source":["import easydict\n","\n","def parse_option():\n","\n","    hostname = socket.gethostname()\n","\n","    # parser = argparse.ArgumentParser('argument for training')\n","\n","    # parser.add_argument('--print_freq', type=int, default=100, help='print frequency')\n","    # parser.add_argument('--tb_freq', type=int, default=500, help='tb frequency')\n","    # parser.add_argument('--save_freq', type=int, default=40, help='save frequency')\n","    # parser.add_argument('--batch_size', type=int, default=64, help='batch_size')\n","    # parser.add_argument('--num_workers', type=int, default=8, help='num of workers to use')\n","    # parser.add_argument('--epochs', type=int, default=1, help='number of training epochs')\n","\n","    # # optimization\n","    # parser.add_argument('--learning_rate', type=float, default=0.05, help='learning rate')\n","    # parser.add_argument('--lr_decay_epochs', type=str, default='150,180,210', help='where to decay lr, can be a list')\n","    # parser.add_argument('--lr_decay_rate', type=float, default=0.1, help='decay rate for learning rate')\n","    # parser.add_argument('--weight_decay', type=float, default=5e-4, help='weight decay')\n","    # parser.add_argument('--momentum', type=float, default=0.9, help='momentum')\n","\n","    # # dataset\n","    # parser.add_argument('--model', type=str, default='resnet110',\n","    #                     choices=['resnet8', 'resnet14', 'resnet20', 'resnet32', 'resnet44', 'resnet56', 'resnet110',\n","    #                              'resnet8x4', 'resnet32x4', 'wrn_16_1', 'wrn_16_2', 'wrn_40_1', 'wrn_40_2',\n","    #                              'vgg8', 'vgg11', 'vgg13', 'vgg16', 'vgg19',\n","    #                              'MobileNetV2', 'ShuffleV1', 'ShuffleV2', ])\n","    # parser.add_argument('--dataset', type=str, default='cifar100', choices=['cifar100'], help='dataset')\n","\n","    # parser.add_argument('-t', '--trial', type=int, default=0, help='the experiment id')\n","\n","    # opt = parser.parse_args()\n","    opt = easydict.EasyDict({\n","            \"print_freq\": 100,\n","            \"tb_freq\": 500,\n","            \"save_freq\": 40,\n","            \"batch_size\": 64,\n","            \"num_workers\": 8,\n","            \"epochs\": 1,\n","            \"learning_rate\": 0.05,\n","            \"lr_decay_epochs\": \"150,180,210\",\n","            \"lr_decay_rate\": 0.1,\n","            \"weight_decay\": 5e-4,\n","            \"momentum\": 0.9,\n","            \"model\": \"resnet110\",\n","            \"dataset\": \"cifar100\",\n","            \"trial\": 0\n","    })\n","    \n","    # set different learning rate from these 4 models\n","    if opt.model in ['MobileNetV2', 'ShuffleV1', 'ShuffleV2']:\n","        opt.learning_rate = 0.01\n","\n","    # set the path according to the environment\n","    if hostname.startswith('visiongpu'):\n","        opt.model_path = '위치'\n","        opt.tb_path = '위치'\n","    else:\n","        opt.model_path = './save/models'\n","        opt.tb_path = './save/tensorboard'\n","\n","    iterations = opt.lr_decay_epochs.split(',')\n","    opt.lr_decay_epochs = list([])\n","    for it in iterations:\n","        opt.lr_decay_epochs.append(int(it))\n","\n","    opt.model_name = '{}_{}_lr_{}_decay_{}_trial_{}'.format(opt.model, opt.dataset, opt.learning_rate,\n","                                                            opt.weight_decay, opt.trial)\n","\n","    opt.tb_folder = os.path.join(opt.tb_path, opt.model_name)\n","    if not os.path.isdir(opt.tb_folder):\n","        os.makedirs(opt.tb_folder)\n","\n","    opt.save_folder = os.path.join(opt.model_path, opt.model_name)\n","    if not os.path.isdir(opt.save_folder):\n","        os.makedirs(opt.save_folder)\n","\n","    return opt"],"metadata":{"id":"pxIyucjNoOs4","executionInfo":{"status":"ok","timestamp":1646493819343,"user_tz":-540,"elapsed":464,"user":{"displayName":"‍이의준[학생](소프트웨어융합대학 컴퓨터공학부)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06208559590960942181"}}},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":["## **learning teacher networks**"],"metadata":{"id":"6r2myrLDofjz"}},{"cell_type":"code","source":["def main():\n","    best_acc = 0\n","\n","    opt = parse_option()\n","\n","    # dataloader\n","    if opt.dataset == 'cifar100':\n","        train_loader, val_loader = get_cifar100_dataloaders(batch_size=opt.batch_size, num_workers=opt.num_workers)\n","        n_cls = 100\n","    else:\n","        raise NotImplementedError(opt.dataset)\n","\n","    # model\n","    model = model_dict[opt.model](num_classes=n_cls)\n","\n","    # optimizer\n","    optimizer = optim.SGD(model.parameters(),\n","                          lr=opt.learning_rate,\n","                          momentum=opt.momentum,\n","                          weight_decay=opt.weight_decay)\n","\n","    criterion = nn.CrossEntropyLoss()\n","\n","    if torch.cuda.is_available():\n","        model = model.cuda()\n","        criterion = criterion.cuda()\n","        cudnn.benchmark = True\n","\n","    # tensorboard\n","    logger = tb_logger.Logger(logdir=opt.tb_folder, flush_secs=2)\n","\n","    # routine\n","    for epoch in range(1, opt.epochs + 1):\n","\n","        adjust_learning_rate(epoch, opt, optimizer)\n","        print(\"==> training...\")\n","\n","        time1 = time.time()\n","        train_acc, train_loss = train_vanilla(epoch, train_loader, model, criterion, optimizer, opt)\n","        time2 = time.time()\n","        print('epoch {}, total time {:.2f}'.format(epoch, time2 - time1))\n","\n","        logger.log_value('train_acc', train_acc, epoch)\n","        logger.log_value('train_loss', train_loss, epoch)\n","\n","        test_acc, test_acc_top5, test_loss = validate(val_loader, model, criterion, opt)\n","\n","        logger.log_value('test_acc', test_acc, epoch)\n","        logger.log_value('test_acc_top5', test_acc_top5, epoch)\n","        logger.log_value('test_loss', test_loss, epoch)\n","\n","        # save the best model\n","        if test_acc > best_acc:\n","            best_acc = test_acc\n","            state = {\n","                'epoch': epoch,\n","                'model': model.state_dict(),\n","                'best_acc': best_acc,\n","                'optimizer': optimizer.state_dict(),\n","            }\n","            save_file = os.path.join(opt.save_folder, '{}_best.pth'.format(opt.model))\n","            print('saving the best model!')\n","            torch.save(state, save_file)\n","\n","        # regular saving\n","        if epoch % opt.save_freq == 0:\n","            print('==> Saving...')\n","            state = {\n","                'epoch': epoch,\n","                'model': model.state_dict(),\n","                'accuracy': test_acc,\n","                'optimizer': optimizer.state_dict(),\n","            }\n","            save_file = os.path.join(opt.save_folder, 'ckpt_epoch_{epoch}.pth'.format(epoch=epoch))\n","            torch.save(state, save_file)\n","\n","    # This best accuracy is only for printing purpose.\n","    # The results reported in the paper/README is from the last epoch.\n","    print('best accuracy:', best_acc)\n","\n","    # save model\n","    state = {\n","        'opt': opt,\n","        'model': model.state_dict(),\n","        'optimizer': optimizer.state_dict(),\n","    }\n","    save_file = os.path.join(opt.save_folder, '{}_last.pth'.format(opt.model))\n","    torch.save(state, save_file)\n","\n","\n","if __name__ == '__main__':\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":568},"id":"om6L7AkWof84","executionInfo":{"status":"error","timestamp":1646494835626,"user_tz":-540,"elapsed":821059,"user":{"displayName":"‍이의준[학생](소프트웨어융합대학 컴퓨터공학부)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06208559590960942181"}},"outputId":"761d5947-560e-440c-d32a-a7f0f692147f"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","==> training...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: [1][0/782]\tTime 3.850 (3.850)\tData 0.464 (0.464)\tLoss 6.2252 (6.2252)\tAcc@1 1.562 (1.562)\tAcc@5 3.125 (3.125)\n","Epoch: [1][100/782]\tTime 3.046 (2.951)\tData 0.004 (0.008)\tLoss 4.5243 (4.7598)\tAcc@1 1.562 (1.532)\tAcc@5 12.500 (6.265)\n","Epoch: [1][200/782]\tTime 2.957 (3.005)\tData 0.004 (0.005)\tLoss 4.4617 (4.6250)\tAcc@1 1.562 (1.959)\tAcc@5 9.375 (8.310)\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-a74e81af3199>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-28-a74e81af3199>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mtime1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_vanilla\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mtime2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch {}, total time {:.2f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-dc2f11adbac8>\u001b[0m in \u001b[0;36mtrain_vanilla\u001b[0;34m(epoch, train_loader, model, criterion, optimizer, opt)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# ===================backward=====================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["# **learning student networks**"],"metadata":{"id":"ImogSrM7tZSn"}},{"cell_type":"code","source":["from __future__ import print_function\n","\n","import os\n","import argparse\n","import socket\n","import time\n","\n","import tensorboard_logger as tb_logger\n","import torch\n","import torch.optim as optim\n","import torch.nn as nn\n","import torch.backends.cudnn as cudnn\n","\n","\n","#from models import model_dict 위에 Traning teacher에서 선언함\n","\n","#from dataset.cifar100 import get_cifar100_dataloaders, get_cifar100_dataloaders_sample\n","\n","#from helper.util import adjust_learning_rate\n","\n","#from distiller_zoo import DistillKL, HintLoss, Attention, Similarity, Correlation, VIDLoss, RKDLoss\n","#from distiller_zoo import PKT, ABLoss, FactorTransfer, KDSVD, FSP, NSTLoss\n","#from crd.criterion import CRDLoss\n","\n","#from helper.loops import train_distill as train, validate\n","#from helper.pretrain import init\n","import ssl\n","ssl._create_default_https_context = ssl._create_unverified_context\n","cudnn.benchmark = True"],"metadata":{"id":"H9t-04Z7tVKR","executionInfo":{"status":"ok","timestamp":1646494944995,"user_tz":-540,"elapsed":323,"user":{"displayName":"‍이의준[학생](소프트웨어융합대학 컴퓨터공학부)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06208559590960942181"}}},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":["###from distiller_zoo import DistillKL, HintLoss, Attention, Similarity, Correlation, VIDLoss, RKDLoss"],"metadata":{"id":"GNtc4ggsu_6w"}},{"cell_type":"code","source":["from __future__ import print_function\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class Attention(nn.Module):\n","    \"\"\"Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks\n","    via Attention Transfer\n","    code: https://github.com/szagoruyko/attention-transfer\"\"\"\n","    def __init__(self, p=2):\n","        super(Attention, self).__init__()\n","        self.p = p\n","\n","    def forward(self, g_s, g_t):\n","        return [self.at_loss(f_s, f_t) for f_s, f_t in zip(g_s, g_t)]\n","\n","    #normalize끼리의 차이를 구하고 다시 l2 norm 계산\n","    def at_loss(self, f_s, f_t):\n","        return (self.at(f_s) - self.at(f_t)).pow(2).mean()\n","\t\n","    #l2 norm 구한 후 normalization\n","    def at(self, f):\n","        return F.normalize(f.pow(self.p).mean(1).view(f.size(0), -1))\n","\n","\n","class DistillKL(nn.Module):\n","    \"\"\"Distilling the Knowledge in a Neural Network\"\"\"\n","    def __init__(self, T):\n","        super(DistillKL, self).__init__()\n","        self.T = T\n","\n","    def forward(self, y_s, y_t):\n","        p_s = F.log_softmax(y_s/self.T, dim=1)\n","        p_t = F.softmax(y_t/self.T, dim=1)\n","        loss = F.kl_div(p_s, p_t, size_average=False) * (self.T**2) / y_s.shape[0]\n","        return loss"],"metadata":{"id":"hfl-b_2BvAdD","executionInfo":{"status":"ok","timestamp":1646495767816,"user_tz":-540,"elapsed":338,"user":{"displayName":"‍이의준[학생](소프트웨어융합대학 컴퓨터공학부)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06208559590960942181"}}},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":["### from crd.criterion import CRDLoss"],"metadata":{"id":"KaPVhbyNvRrR"}},{"cell_type":"markdown","source":["from .memory import ContrastMemory"],"metadata":{"id":"CuxCdV5Cv9mE"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","import math\n","\n","\n","class ContrastMemory(nn.Module):\n","    \"\"\"\n","    memory buffer that supplies large amount of negative samples.\n","    \"\"\"\n","    def __init__(self, inputSize, outputSize, K, T=0.07, momentum=0.5):\n","        super(ContrastMemory, self).__init__()\n","        self.nLem = outputSize\n","        self.unigrams = torch.ones(self.nLem)\n","        self.multinomial = AliasMethod(self.unigrams)\n","        self.multinomial.cuda()\n","        self.K = K\n","\n","        self.register_buffer('params', torch.tensor([K, T, -1, -1, momentum]))\n","        stdv = 1. / math.sqrt(inputSize / 3)\n","        self.register_buffer('memory_v1', torch.rand(outputSize, inputSize).mul_(2 * stdv).add_(-stdv))\n","        self.register_buffer('memory_v2', torch.rand(outputSize, inputSize).mul_(2 * stdv).add_(-stdv))\n","\n","    def forward(self, v1, v2, y, idx=None):\n","        K = int(self.params[0].item())\n","        T = self.params[1].item()\n","        Z_v1 = self.params[2].item()\n","        Z_v2 = self.params[3].item()\n","\n","        momentum = self.params[4].item()\n","        batchSize = v1.size(0)\n","        outputSize = self.memory_v1.size(0)\n","        inputSize = self.memory_v1.size(1)\n","\n","        # original score computation\n","        if idx is None:\n","            idx = self.multinomial.draw(batchSize * (self.K + 1)).view(batchSize, -1)\n","            idx.select(1, 0).copy_(y.data)\n","        # sample\n","        weight_v1 = torch.index_select(self.memory_v1, 0, idx.view(-1)).detach()\n","        weight_v1 = weight_v1.view(batchSize, K + 1, inputSize)\n","        out_v2 = torch.bmm(weight_v1, v2.view(batchSize, inputSize, 1))\n","        out_v2 = torch.exp(torch.div(out_v2, T))\n","        # sample\n","        weight_v2 = torch.index_select(self.memory_v2, 0, idx.view(-1)).detach()\n","        weight_v2 = weight_v2.view(batchSize, K + 1, inputSize)\n","        out_v1 = torch.bmm(weight_v2, v1.view(batchSize, inputSize, 1))\n","        out_v1 = torch.exp(torch.div(out_v1, T))\n","\n","        # set Z if haven't been set yet\n","        if Z_v1 < 0:\n","            self.params[2] = out_v1.mean() * outputSize\n","            Z_v1 = self.params[2].clone().detach().item()\n","            print(\"normalization constant Z_v1 is set to {:.1f}\".format(Z_v1))\n","        if Z_v2 < 0:\n","            self.params[3] = out_v2.mean() * outputSize\n","            Z_v2 = self.params[3].clone().detach().item()\n","            print(\"normalization constant Z_v2 is set to {:.1f}\".format(Z_v2))\n","\n","        # compute out_v1, out_v2\n","        out_v1 = torch.div(out_v1, Z_v1).contiguous()\n","        out_v2 = torch.div(out_v2, Z_v2).contiguous()\n","\n","        # update memory\n","        with torch.no_grad():\n","            l_pos = torch.index_select(self.memory_v1, 0, y.view(-1))\n","            l_pos.mul_(momentum)\n","            l_pos.add_(torch.mul(v1, 1 - momentum))\n","            l_norm = l_pos.pow(2).sum(1, keepdim=True).pow(0.5)\n","            updated_v1 = l_pos.div(l_norm)\n","            self.memory_v1.index_copy_(0, y, updated_v1)\n","\n","            ab_pos = torch.index_select(self.memory_v2, 0, y.view(-1))\n","            ab_pos.mul_(momentum)\n","            ab_pos.add_(torch.mul(v2, 1 - momentum))\n","            ab_norm = ab_pos.pow(2).sum(1, keepdim=True).pow(0.5)\n","            updated_v2 = ab_pos.div(ab_norm)\n","            self.memory_v2.index_copy_(0, y, updated_v2)\n","\n","        return out_v1, out_v2"],"metadata":{"id":"IRyv7ZQ6v_n-","executionInfo":{"status":"ok","timestamp":1646495010417,"user_tz":-540,"elapsed":896,"user":{"displayName":"‍이의준[학생](소프트웨어융합대학 컴퓨터공학부)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06208559590960942181"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch import nn\n","#from .memory import ContrastMemory\n","\n","eps = 1e-7\n","\n","\n","class CRDLoss(nn.Module):\n","    \"\"\"CRD Loss function\n","    includes two symmetric parts:\n","    (a) using teacher as anchor, choose positive and negatives over the student side\n","    (b) using student as anchor, choose positive and negatives over the teacher side\n","    Args:\n","        opt.s_dim: the dimension of student's feature\n","        opt.t_dim: the dimension of teacher's feature\n","        opt.feat_dim: the dimension of the projection space\n","        opt.nce_k: number of negatives paired with each positive\n","        opt.nce_t: the temperature\n","        opt.nce_m: the momentum for updating the memory buffer\n","        opt.n_data: the number of samples in the training set, therefor the memory buffer is: opt.n_data x opt.feat_dim\n","    \"\"\"\n","    def __init__(self, opt):\n","        super(CRDLoss, self).__init__()\n","        self.embed_s = Embed(opt.s_dim, opt.feat_dim)\n","        self.embed_t = Embed(opt.t_dim, opt.feat_dim)\n","        self.contrast = ContrastMemory(opt.feat_dim, opt.n_data, opt.nce_k, opt.nce_t, opt.nce_m)\n","        self.criterion_t = ContrastLoss(opt.n_data)\n","        self.criterion_s = ContrastLoss(opt.n_data)\n","\n","    def forward(self, f_s, f_t, idx, contrast_idx=None):\n","        \"\"\"\n","        Args:\n","            f_s: the feature of student network, size [batch_size, s_dim]\n","            f_t: the feature of teacher network, size [batch_size, t_dim]\n","            idx: the indices of these positive samples in the dataset, size [batch_size]\n","            contrast_idx: the indices of negative samples, size [batch_size, nce_k]\n","        Returns:\n","            The contrastive loss\n","        \"\"\"\n","        f_s = self.embed_s(f_s)\n","        f_t = self.embed_t(f_t)\n","        out_s, out_t = self.contrast(f_s, f_t, idx, contrast_idx)\n","        s_loss = self.criterion_s(out_s)\n","        t_loss = self.criterion_t(out_t)\n","        loss = s_loss + t_loss\n","        return loss\n","\n","class ContrastLoss(nn.Module):\n","    \"\"\"\n","    contrastive loss, corresponding to Eq (18)\n","    \"\"\"\n","    def __init__(self, n_data):\n","        super(ContrastLoss, self).__init__()\n","        self.n_data = n_data\n","\n","    def forward(self, x):\n","        bsz = x.shape[0]\n","        m = x.size(1) - 1\n","\n","        # noise distribution\n","        Pn = 1 / float(self.n_data)\n","\n","        # loss for positive pair\n","        P_pos = x.select(1, 0)\n","        log_D1 = torch.div(P_pos, P_pos.add(m * Pn + eps)).log_()\n","\n","        # loss for K negative pair\n","        P_neg = x.narrow(1, 1, m)\n","        log_D0 = torch.div(P_neg.clone().fill_(m * Pn), P_neg.add(m * Pn + eps)).log_()\n","\n","        loss = - (log_D1.sum(0) + log_D0.view(-1, 1).sum(0)) / bsz\n","\n","        return loss\n","\n","\n","class Embed(nn.Module):\n","    \"\"\"Embedding module\"\"\"\n","    def __init__(self, dim_in=1024, dim_out=128):\n","        super(Embed, self).__init__()\n","        self.linear = nn.Linear(dim_in, dim_out)\n","        self.l2norm = Normalize(2)\n","\n","    def forward(self, x):\n","        x = x.view(x.shape[0], -1)\n","        x = self.linear(x)\n","        x = self.l2norm(x)\n","        return x\n","\n","\n","class Normalize(nn.Module):\n","    \"\"\"normalization layer\"\"\"\n","    def __init__(self, power=2):\n","        super(Normalize, self).__init__()\n","        self.power = power\n","\n","    def forward(self, x):\n","        norm = x.pow(self.power).sum(1, keepdim=True).pow(1. / self.power)\n","        out = x.div(norm)\n","        return out"],"metadata":{"id":"ne3ntQQXvSWm","executionInfo":{"status":"ok","timestamp":1646495017329,"user_tz":-540,"elapsed":510,"user":{"displayName":"‍이의준[학생](소프트웨어융합대학 컴퓨터공학부)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06208559590960942181"}}},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":["### from helper.pretrain import init"],"metadata":{"id":"r1rk9KaRvkzb"}},{"cell_type":"code","source":["from __future__ import print_function, division\n","\n","import time\n","import sys\n","import torch\n","import torch.optim as optim\n","import torch.backends.cudnn as cudnn\n","#from .util import AverageMeter\n","\n","\n","def init(model_s, model_t, init_modules, criterion, train_loader, logger, opt):\n","    model_t.eval()\n","    model_s.eval()\n","    init_modules.train()\n","\n","    if torch.cuda.is_available():\n","        model_s.cuda()\n","        model_t.cuda()\n","        init_modules.cuda()\n","        cudnn.benchmark = True\n","\n","    if opt.model_s in ['resnet8', 'resnet14', 'resnet20', 'resnet32', 'resnet44', 'resnet56', 'resnet110',\n","                       'resnet8x4', 'resnet32x4', 'wrn_16_1', 'wrn_16_2', 'wrn_40_1', 'wrn_40_2'] and \\\n","            opt.distill == 'factor':\n","        lr = 0.01\n","    else:\n","        lr = opt.learning_rate\n","    optimizer = optim.SGD(init_modules.parameters(),\n","                          lr=lr,\n","                          momentum=opt.momentum,\n","                          weight_decay=opt.weight_decay)\n","\n","    batch_time = AverageMeter()\n","    data_time = AverageMeter()\n","    losses = AverageMeter()\n","    for epoch in range(1, opt.init_epochs + 1):\n","        batch_time.reset()\n","        data_time.reset()\n","        losses.reset()\n","        end = time.time()\n","        for idx, data in enumerate(train_loader):\n","            if opt.distill in ['crd']:\n","                input, target, index, contrast_idx = data\n","            else:\n","                input, target, index = data\n","            data_time.update(time.time() - end)\n","\n","            input = input.float()\n","            if torch.cuda.is_available():\n","                input = input.cuda()\n","                target = target.cuda()\n","                index = index.cuda()\n","                if opt.distill in ['crd']:\n","                    contrast_idx = contrast_idx.cuda()\n","\n","            # ============= forward ==============\n","            preact = (opt.distill == 'abound')\n","            feat_s, _ = model_s(input, is_feat=True, preact=preact)\n","            with torch.no_grad():\n","                feat_t, _ = model_t(input, is_feat=True, preact=preact)\n","                feat_t = [f.detach() for f in feat_t]\n","\n","            if opt.distill == 'abound':\n","                g_s = init_modules[0](feat_s[1:-1])\n","                g_t = feat_t[1:-1]\n","                loss_group = criterion(g_s, g_t)\n","                loss = sum(loss_group)\n","            elif opt.distill == 'factor':\n","                f_t = feat_t[-2]\n","                _, f_t_rec = init_modules[0](f_t)\n","                loss = criterion(f_t_rec, f_t)\n","            elif opt.distill == 'fsp':\n","                loss_group = criterion(feat_s[:-1], feat_t[:-1])\n","                loss = sum(loss_group)\n","            else:\n","                raise NotImplemented('Not supported in init training: {}'.format(opt.distill))\n","\n","            losses.update(loss.item(), input.size(0))\n","\n","            # ===================backward=====================\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            batch_time.update(time.time() - end)\n","            end = time.time()\n","\n","        # end of epoch\n","        logger.log_value('init_train_loss', losses.avg, epoch)\n","        print('Epoch: [{0}/{1}]\\t'\n","              'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n","              'losses: {losses.val:.3f} ({losses.avg:.3f})'.format(\n","               epoch, opt.init_epochs, batch_time=batch_time, losses=losses))\n","        sys.stdout.flush()"],"metadata":{"id":"FxgStt4OvlEl","executionInfo":{"status":"ok","timestamp":1646494927595,"user_tz":-540,"elapsed":316,"user":{"displayName":"‍이의준[학생](소프트웨어융합대학 컴퓨터공학부)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06208559590960942181"}}},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":["## set student's arguments"],"metadata":{"id":"ypyBSj2nwFbR"}},{"cell_type":"code","source":["def parse_option2():\n","\n","    hostname = socket.gethostname()\n","\n","    # parser = argparse.ArgumentParser('argument for training')\n","\n","    # parser.add_argument('--print_freq', type=int, default=100, help='print frequency')\n","    # parser.add_argument('--tb_freq', type=int, default=500, help='tb frequency')\n","    # parser.add_argument('--save_freq', type=int, default=40, help='save frequency')\n","    # parser.add_argument('--batch_size', type=int, default=64, help='batch_size')\n","    # parser.add_argument('--num_workers', type=int, default=8, help='num of workers to use')\n","    # parser.add_argument('--epochs', type=int, default=5, help='number of training epochs')\n","    # parser.add_argument('--init_epochs', type=int, default=30, help='init training for two-stage methods')\n","\n","    # # optimization\n","    # parser.add_argument('--learning_rate', type=float, default=0.05, help='learning rate')\n","    # parser.add_argument('--lr_decay_epochs', type=str, default='150,180,210', help='where to decay lr, can be a list')\n","    # parser.add_argument('--lr_decay_rate', type=float, default=0.1, help='decay rate for learning rate')\n","    # parser.add_argument('--weight_decay', type=float, default=5e-4, help='weight decay')\n","    # parser.add_argument('--momentum', type=float, default=0.9, help='momentum')\n","\n","    # # dataset\n","    # parser.add_argument('--dataset', type=str, default='cifar100', choices=['cifar100'], help='dataset')\n","\n","    # # model\n","    # parser.add_argument('--model_s', type=str, default='resnet8',\n","    #                     choices=['resnet8', 'resnet14', 'resnet20', 'resnet32', 'resnet44', 'resnet56', 'resnet110',\n","    #                              'resnet8x4', 'resnet32x4', 'wrn_16_1', 'wrn_16_2', 'wrn_40_1', 'wrn_40_2',\n","    #                              'vgg8', 'vgg11', 'vgg13', 'vgg16', 'vgg19', 'ResNet50',\n","    #                              'MobileNetV2', 'ShuffleV1', 'ShuffleV2'])\n","    # parser.add_argument('--path_t', type=str, default='teacher weight 저장 위치', help='teacher model snapshot')\n","\n","    # # distillation\n","    # parser.add_argument('--distill', type=str, default='attention', choices=['kd', 'hint', 'attention', 'similarity',\n","    #                                                                   'correlation', 'vid', 'crd', 'kdsvd', 'fsp',\n","    #                                                                   'rkd', 'pkt', 'abound', 'factor', 'nst'])\n","    # parser.add_argument('--trial', type=str, default='1', help='trial id')\n","\n","    # parser.add_argument('-r', '--gamma', type=float, default=0.9, help='weight for classification')\n","    # parser.add_argument('-a', '--alpha', type=float, default=0.1, help='weight balance for KD')\n","    # parser.add_argument('-b', '--beta', type=float, default=0.1, help='weight balance for other losses')\n","\n","    # # KL distillation\n","    # parser.add_argument('--kd_T', type=float, default=4, help='temperature for KD distillation')\n","\n","    # # NCE distillation\n","    # parser.add_argument('--feat_dim', default=128, type=int, help='feature dimension')\n","    # parser.add_argument('--mode', default='exact', type=str, choices=['exact', 'relax'])\n","    # parser.add_argument('--nce_k', default=16384, type=int, help='number of negative samples for NCE')\n","    # parser.add_argument('--nce_t', default=0.07, type=float, help='temperature parameter for softmax')\n","    # parser.add_argument('--nce_m', default=0.5, type=float, help='momentum for non-parametric updates')\n","\n","    # # hint layer\n","    # parser.add_argument('--hint_layer', default=2, type=int, choices=[0, 1, 2, 3, 4])\n","\n","    # opt = parser.parse_args()\n","\n","    opt = easydict.EasyDict({\n","            \"print_freq\": 100,\n","            \"tb_freq\": 500,\n","            \"save_freq\": 40,\n","            \"batch_size\": 64,\n","            \"num_workers\": 8,\n","            \"epochs\": 5,\n","            \"init_epochs\": 30,\n","            \"learning_rate\": 0.05,\n","            \"lr_decay_epochs\": \"150,180,210\",\n","            \"lr_decay_rate\": 0.1,\n","            \"weight_decay\": 5e-4,\n","            \"momentum\": 0.9,\n","            \"model_s\": \"resnet8\",\n","            \"dataset\": \"cifar100\",\n","            \"distill\": 'attention',\n","            \"path_t\": 'teacher weight 저장 위치',\n","            \"trial\": 1,\n","            \"gamma\": 0.9,\n","            \"alpha\": 0.1,\n","            \"beta\": 0.1,\n","            \"kd_T\": 4,\n","            \"feat_dim\": 128,\n","            \"mode\": 'exact',\n","            \"nce_k\": 16384,\n","            \"nce_t\": 0.07,\n","            \"nce_m\": 0.5,\n","            \"hint_layer\": 2\n","    })\n","\n","    # set different learning rate from these 4 models\n","    if opt.model_s in ['MobileNetV2', 'ShuffleV1', 'ShuffleV2']:\n","        opt.learning_rate = 0.01\n","\n","    # set the path according to the environment\n","    if hostname.startswith('visiongpu'):\n","        opt.model_path = '/path/to/my/student_model'\n","        opt.tb_path = '/path/to/my/student_tensorboards'\n","    else:\n","        opt.model_path = './save/student_model'\n","        opt.tb_path = './save/student_tensorboards'\n","\n","    iterations = opt.lr_decay_epochs.split(',')\n","    opt.lr_decay_epochs = list([])\n","    for it in iterations:\n","        opt.lr_decay_epochs.append(int(it))\n","\n","    opt.model_t = get_teacher_name(opt.path_t)\n","\n","    opt.model_name = 'S:{}_T:{}_{}_{}_r:{}_a:{}_b:{}_{}'.format(opt.model_s, opt.model_t, opt.dataset, opt.distill,\n","                                                                opt.gamma, opt.alpha, opt.beta, opt.trial)\n","\n","    opt.tb_folder = os.path.join(opt.tb_path, opt.model_name)\n","    if not os.path.isdir(opt.tb_folder):\n","        os.makedirs(opt.tb_folder)\n","\n","    opt.save_folder = os.path.join(opt.model_path, opt.model_name)\n","    if not os.path.isdir(opt.save_folder):\n","        os.makedirs(opt.save_folder)\n","\n","    return opt"],"metadata":{"id":"Kvn-X-C4wE-s","executionInfo":{"status":"ok","timestamp":1646495519370,"user_tz":-540,"elapsed":341,"user":{"displayName":"‍이의준[학생](소프트웨어융합대학 컴퓨터공학부)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06208559590960942181"}}},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":["## **load teacher**"],"metadata":{"id":"VeBT7diDxtGW"}},{"cell_type":"code","source":["def get_teacher_name(model_path):\n","    \"\"\"parse teacher name\"\"\"\n","    segments = model_path.split('/')[-2].split('_')\n","    if segments[0] != 'wrn':\n","        return segments[0]\n","    else:\n","        return segments[0] + '_' + segments[1] + '_' + segments[2]\n","\n","\n","def load_teacher(model_path, n_cls):\n","    print('==> loading teacher model')\n","    model_t = get_teacher_name(model_path)\n","    model = model_dict[model_t](num_classes=n_cls)\n","    model.load_state_dict(torch.load(model_path)['model'])\n","    print('==> done')\n","    return model"],"metadata":{"id":"N-WX_lESxvR7","executionInfo":{"status":"ok","timestamp":1646495465847,"user_tz":-540,"elapsed":493,"user":{"displayName":"‍이의준[학생](소프트웨어융합대학 컴퓨터공학부)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06208559590960942181"}}},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":["## **learning student networks**"],"metadata":{"id":"4PZknsj_xyUI"}},{"cell_type":"code","source":["def main():\n","    best_acc = 0\n","\n","    opt = parse_option2()\n","\n","    # tensorboard logger\n","    logger = tb_logger.Logger(logdir=opt.tb_folder, flush_secs=2)\n","\n","    # dataloader\n","    if opt.dataset == 'cifar100':\n","        if opt.distill in ['crd']:\n","            train_loader, val_loader, n_data = get_cifar100_dataloaders_sample(batch_size=opt.batch_size,\n","                                                                               num_workers=opt.num_workers,\n","                                                                               k=opt.nce_k,\n","                                                                               mode=opt.mode)\n","        else:\n","            train_loader, val_loader, n_data = get_cifar100_dataloaders(batch_size=opt.batch_size,\n","                                                                        num_workers=opt.num_workers,\n","                                                                        is_instance=True)\n","        n_cls = 100\n","    else:\n","        raise NotImplementedError(opt.dataset)\n","\n","    # model\n","    model_t = load_teacher(opt.path_t, n_cls)\n","    model_s = model_dict[opt.model_s](num_classes=n_cls)\n","\n","    data = torch.randn(2, 3, 32, 32)\n","    model_t.eval()\n","    model_s.eval()\n","    feat_t, _ = model_t(data, is_feat=True)\n","    feat_s, _ = model_s(data, is_feat=True)\n","\n","    module_list = nn.ModuleList([])\n","    module_list.append(model_s)\n","    trainable_list = nn.ModuleList([])\n","    trainable_list.append(model_s)\n","\n","    criterion_cls = nn.CrossEntropyLoss()\n","    criterion_div = DistillKL(opt.kd_T)\n","    criterion_kd = Attention()\n","    \n","\n","    criterion_list = nn.ModuleList([])\n","    criterion_list.append(criterion_cls)    # classification loss\n","    criterion_list.append(criterion_div)    # KL divergence loss, original knowledge distillation\n","    criterion_list.append(criterion_kd)     # other knowledge distillation loss\n","\n","    # optimizer\n","    optimizer = optim.SGD(trainable_list.parameters(),\n","                          lr=opt.learning_rate,\n","                          momentum=opt.momentum,\n","                          weight_decay=opt.weight_decay)\n","\n","    # append teacher after optimizer to avoid weight_decay\n","    module_list.append(model_t)\n","\n","    if torch.cuda.is_available():\n","        module_list.cuda()\n","        criterion_list.cuda()\n","        cudnn.benchmark = True\n","\n","    # validate teacher accuracy\n","    teacher_acc, _, _ = validate(val_loader, model_t, criterion_cls, opt)\n","    print('teacher accuracy: ', teacher_acc)\n","\n","    # routine\n","    for epoch in range(1, opt.epochs + 1):\n","\n","        adjust_learning_rate(epoch, opt, optimizer)\n","        print(\"==> training...\")\n","\n","        time1 = time.time()\n","        train_acc, train_loss = train_distill(epoch, train_loader, module_list, criterion_list, optimizer, opt)\n","        time2 = time.time()\n","        print('epoch {}, total time {:.2f}'.format(epoch, time2 - time1))\n","\n","        logger.log_value('train_acc', train_acc, epoch)\n","        logger.log_value('train_loss', train_loss, epoch)\n","\n","        test_acc, tect_acc_top5, test_loss = validate(val_loader, model_s, criterion_cls, opt)\n","\n","        logger.log_value('test_acc', test_acc, epoch)\n","        logger.log_value('test_loss', test_loss, epoch)\n","        logger.log_value('test_acc_top5', tect_acc_top5, epoch)\n","\n","        # save the best model\n","        if test_acc > best_acc:\n","            best_acc = test_acc\n","            state = {\n","                'epoch': epoch,\n","                'model': model_s.state_dict(),\n","                'best_acc': best_acc,\n","            }\n","            save_file = os.path.join(opt.save_folder, '{}_best.pth'.format(opt.model_s))\n","            print('saving the best model!')\n","            torch.save(state, save_file)\n","\n","        # regular saving\n","        if epoch % opt.save_freq == 0:\n","            print('==> Saving...')\n","            state = {\n","                'epoch': epoch,\n","                'model': model_s.state_dict(),\n","                'accuracy': test_acc,\n","            }\n","            save_file = os.path.join(opt.save_folder, 'ckpt_epoch_{epoch}.pth'.format(epoch=epoch))\n","            torch.save(state, save_file)\n","\n","    # This best accuracy is only for printing purpose.\n","    # The results reported in the paper/README is from the last epoch. \n","    print('best accuracy:', best_acc)\n","\n","    # save model\n","    state = {\n","        'opt': opt,\n","        'model': model_s.state_dict(),\n","    }\n","    save_file = os.path.join(opt.save_folder, '{}_last.pth'.format(opt.model_s))\n","    torch.save(state, save_file)\n","\n","\n","if __name__ == '__main__':\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":364},"id":"F86ukxMJxyir","executionInfo":{"status":"error","timestamp":1646495772234,"user_tz":-540,"elapsed":319,"user":{"displayName":"‍이의준[학생](소프트웨어융합대학 컴퓨터공학부)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06208559590960942181"}},"outputId":"72ef1659-3f79-46bd-ce6f-91594d515cea"},"execution_count":48,"outputs":[{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-48-1a10db3e7fea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-48-1a10db3e7fea>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mbest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_option2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# tensorboard logger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-43-4950d833d347>\u001b[0m in \u001b[0;36mparse_option2\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_decay_epochs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_teacher_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     opt.model_name = 'S:{}_T:{}_{}_{}_r:{}_a:{}_b:{}_{}'.format(opt.model_s, opt.model_t, opt.dataset, opt.distill,\n","\u001b[0;32m<ipython-input-41-af90a7509a55>\u001b[0m in \u001b[0;36mget_teacher_name\u001b[0;34m(model_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_teacher_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"parse teacher name\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msegments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msegments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'wrn'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msegments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"]}]}]}